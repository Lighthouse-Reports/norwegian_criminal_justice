{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script: Preprocess Data \n",
    "Author: Gabriel Geiger\n",
    "Date: 17-01-2024 \n",
    "<br>\n",
    "\n",
    "Description: <br>\n",
    "@input: Raw json files representing databases containing judgements, defendants and lay judges. \n",
    "@output: A merged dataframe where all nested data is flattened. Each row is a defendant with their corresponding case data (outcome, lay judges etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "os.chdir(\"/Users/justin-casimirbraun/GitHub/norwegian_lay_judges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Defendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defendant dataframe shape (8094, 24)\n"
     ]
    }
   ],
   "source": [
    "def load_defendants(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "    return df\n",
    "\n",
    "defendant_df = load_defendants(os.getcwd() + \"/00_raw_data/hidden/lr_tiltalte.json\")\n",
    "\n",
    "# Extract ID for defendants \n",
    "defendant_df[\"_id\"] = defendant_df[\"_id\"].apply(lambda id_dict: id_dict[\"$oid\"])\n",
    "\n",
    "# Add defendant prefix to all columns to avoid column collision with lay judges \n",
    "defendant_df = defendant_df.add_prefix(\"defendant_\")\n",
    "\n",
    "print(\"Defendant dataframe shape\",defendant_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Dataframe shape (9019, 8)\n"
     ]
    }
   ],
   "source": [
    "def load_cases(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "\n",
    "    return df\n",
    "\n",
    "cases_df = load_cases(os.getcwd() + \"/00_raw_data/hidden/lr_dommer.json\")\n",
    "\n",
    "# Extract id for cases \n",
    "cases_df[\"_id\"] = cases_df[\"_id\"].apply(\n",
    "    lambda id : id[\"$oid\"]\n",
    ")\n",
    "\n",
    "print(\"Case Dataframe shape\",cases_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Data Validation \n",
    "\n",
    "Run data validation steps and throw out any rows that violate business rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 cases have been dropped because they did not have a judgement.\n",
      "127 cases have been dropped because they did not have one judge.\n",
      "36 cases have been dropped because they did not have two lay judges.\n",
      "11 cases have been dropped because they did not have a defendant.\n",
      "Shape (8767, 9)\n"
     ]
    }
   ],
   "source": [
    "# All cases should have a judgement, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"dom\"],inplace=True)\n",
    "print(\"{n} cases have been dropped because they did not have a judgement.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have one judge, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"fagdommere\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"fagdommere\"].apply(\n",
    "    lambda judge_array: len(judge_array) == 1)\n",
    "]\n",
    "\n",
    "# There appears to have been some parsing on the Norwegians-side. \n",
    "cases_df = cases_df[cases_df[\"fagdommere\"].apply(\n",
    "    lambda judge_array : type(judge_array) == list\n",
    ")]\n",
    "print(\"{n} cases have been dropped because they did not have one judge.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have two lay judges, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"meddommere\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"meddommere\"].apply(\n",
    "    lambda lay_array: len(lay_array) == 2)\n",
    "]\n",
    "print(\"{n} cases have been dropped because they did not have two lay judges.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have at least one defendant\n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"tiltalte\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"tiltalte\"].apply(\n",
    "    lambda lay_array: len(lay_array) > 0)\n",
    "]\n",
    "print(\"{n} cases have been dropped because they did not have a defendant.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# Reset the index. \n",
    "cases_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Shape\",cases_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Case DataFrame with Defendants DataFrame \n",
    "\n",
    "Each defendant in the defendant database has an ID. Each case in the case dataframe has an array of defendant ids. Ultimately we want each row in our final dataframe to be a defendant and their corresponding case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113 rows have been dropped because no matching defendant was found.\n",
      "Shape (8175, 34)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Takes the raw \n",
    "\"\"\"\n",
    "def parse_defendant_ids(raw_defendant_data:dict) -> list : \n",
    "    defendant_ids = []\n",
    "\n",
    "    for defendant in raw_defendant_data : \n",
    "        if \"motpart_id\" in defendant : \n",
    "            if \"$oid\" in defendant[\"motpart_id\"] : \n",
    "                defendant_ids.append(defendant[\"motpart_id\"]['$oid'])\n",
    "    \n",
    "    return defendant_ids\n",
    "        \n",
    "# Create a clean list of defendant ids associated with the case \n",
    "cases_df[\"defendant_ids\"] = cases_df[\"tiltalte\"].apply(parse_defendant_ids)\n",
    "\n",
    "# Explode the 'defendant_ids' column in cases_df to create separate rows for each defendant\n",
    "cases_exploded = cases_df.explode('defendant_ids')\n",
    "\n",
    "# Merge the exploded cases_df with defendants_df based on defendant_id\n",
    "merged_df = pd.merge(cases_exploded, defendant_df, left_on='defendant_ids', right_on='defendant__id', how='left',suffixes=('_case', '_defendant'))\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Drop all cases without a defendant. Unfortunately not every defendant has an ID \n",
    "length_before = len(merged_df)\n",
    "merged_df.dropna(subset=[\"defendant__id\"],inplace=True)\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "print(\"{i} rows have been dropped because no matching defendant was found.\".format(i=length_before - len(merged_df)))\n",
    "\n",
    "print(\"Shape\",merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Judgement Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dom column\n",
    "judgement_df = pd.json_normalize(data=merged_df[\"dom\"],meta=[\"dømt\",\"frifunnet\",\"uvisst\"])\n",
    "\n",
    "# Create a dummy variable for frifunnet (aquitted) \n",
    "judgement_df[\"judgement_aquitted\"] = judgement_df[\"frifunnet\"].apply(\n",
    "    lambda a : False if isinstance(a,float) else True)\n",
    "\n",
    "# Create a dummy variable for uvisst (unknown) \n",
    "judgement_df[\"judgement_unknown\"] = judgement_df[\"uvisst\"].apply(\n",
    "    lambda u : False if isinstance(u,float) else True)\n",
    "\n",
    "# Create a dummy variable for dømt (convicted)\n",
    "judgement_df[\"judgement_convicted\"] = judgement_df[\"dømt\"].apply(\n",
    "    lambda u : False if isinstance(u,float) else True)\n",
    "\n",
    "# # Flatten sections in dømt (conviction)\n",
    "# judgement_df[\"dømt\"] = judgement_df[\"dømt\"].apply(flatten_judgement_sections)\n",
    "\n",
    "# # Explode it so we have all sections in the database in one place and take unique\n",
    "# section_counts = judgement_df[\"dømt\"].explode().value_counts()\n",
    "\n",
    "# # Filter for all values above threshold (Note: Set to 0 for now because we want to include everything)\n",
    "# filtered_values = section_counts[section_counts > 0].index.tolist()\n",
    "\n",
    "# # Turn all sections with a count above the threshold into a dummy variable \n",
    "# for section in filtered_values:\n",
    "#     judgement_df[\"convicted_\" + section] = judgement_df['dømt'].apply(\n",
    "#          dummify_sections,args=(section,)\n",
    "#     ).astype(int)\n",
    "\n",
    "# # Put everything else into other \n",
    "# judgement_df[\"convicted_other\"] = judgement_df['dømt'].apply(\n",
    "#     lambda row_sections: False if isinstance(row_sections,float) else any(val not in filtered_values for val in row_sections)).astype(int)\n",
    "\n",
    "# Merge the dataframes \n",
    "length_before_merge = len(merged_df)\n",
    "merged_df = pd.merge(merged_df, judgement_df, left_index=True, right_index=True, how='inner')\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Make sure no merging errors \n",
    "assert length_before_merge == len(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profesional Judges \n",
    "\n",
    "The only variable here behind function (which is always just profesional judge) is whether the profesional judge is or isn't in the majority. Note, this variable is n/a if the decision is unanimous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_judge_data(raw_judge_data:list) : \n",
    "    judge_dict = raw_judge_data[0]\n",
    "\n",
    "    if \"flertall_eller_mindretall\" in judge_dict : \n",
    "        if judge_dict[\"flertall_eller_mindretall\"] == \"flertall\" : \n",
    "            return \"Majority\"\n",
    "\n",
    "        else : \n",
    "            return \"Minority\"\n",
    "    \n",
    "    else : \n",
    "        return None \n",
    "\n",
    "merged_df[\"judge_majority_or_minority\"] = merged_df[\"fagdommere\"].apply(parse_judge_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indictment\n",
    "\n",
    "The sections under the indictment are in nested json of an arbitrary depth, so it's a bit tricky to unpack it all. Ultimately, we want to turn all indictment sections into dummy variables. \n",
    "\n",
    "An important note: Not all the sections in the indictment nessicairly pertain to all defendants in the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Recursively flatten the nested json \n",
    "\"\"\"\n",
    "def flatten_judgement_sections(nested_list) -> list:\n",
    "    if isinstance(nested_list,float) : \n",
    "        return nested_list \n",
    "\n",
    "    # If the judgement is unknown it is labeled \"Fant ikke ord\"\n",
    "    if isinstance(nested_list,str) : \n",
    "        return [nested_list]\n",
    "    \n",
    "    flattened_list = []\n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_judgement_sections(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "\n",
    "    return flattened_list\n",
    "\n",
    "def dummify_sections(row_sections:list,section:str) -> bool : \n",
    "    if isinstance(row_sections,float) : \n",
    "        return False \n",
    "    \n",
    "    elif section in row_sections : \n",
    "        return True \n",
    "\n",
    "    else : \n",
    "        return False \n",
    "    \n",
    "# Flatten sections in tiltale (indictment)\n",
    "merged_df[\"flattened_charges\"] = merged_df[\"tiltale\"].apply(flatten_judgement_sections)\n",
    "\n",
    "# Explode it so we have all sections in the database in one place and take unique\n",
    "section_counts = merged_df[\"flattened_charges\"].explode().value_counts()\n",
    "\n",
    "# Filter for all values above threshold (Note: Set to 0 for now because we want to include everything)\n",
    "filtered_values = section_counts[section_counts > 0].index.tolist()\n",
    "\n",
    "# Turn all sections with a count above the threshold into a dummy variable (For now all sections)\n",
    "for section in filtered_values:\n",
    "    merged_df[\"charged_\" + section] = merged_df['flattened_charges'].apply(\n",
    "         dummify_sections,args=(section,)\n",
    "    ).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching Defendant Data\n",
    "\n",
    "The case data contains an array called 'defendants' with some information about each defendant in the case (punishment, whether they were aquitted, etc.). For our purposes, we only want each row to have the relevant case defendant data and punishment, not the data for all the other defendants in the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Matches the nested defendant data with the defendant matched to this row. \n",
    "\"\"\"\n",
    "def match_defendant_data(row) -> dict : \n",
    "    defendant_list = row[\"tiltalte\"]\n",
    "\n",
    "    for defendant in defendant_list: \n",
    "        if \"motpart_id\" in defendant : \n",
    "            if defendant[\"motpart_id\"][\"$oid\"] == row[\"defendant__id\"] : \n",
    "                return defendant\n",
    "\n",
    "\"\"\"\n",
    "Function for flattening the nested json data related to the defendant. \n",
    "Creates two intermediate variables: convicted_raw and punishment_raw that are unpacked \n",
    "seperately \n",
    "\"\"\"\n",
    "def parse_defendant_data(raw_defendent_data:dict) : \n",
    "    defendant_data = {\"convicted_raw\":None,\n",
    "                      \"defendant_aquited\":None,\n",
    "                      \"punishment_raw\":None}\n",
    "\n",
    "    if \"dømt\" in raw_defendent_data : \n",
    "        defendant_data[\"convicted_raw\"] = raw_defendent_data[\"dømt\"]\n",
    "    \n",
    "    if \"frifunnet\" in raw_defendent_data : \n",
    "        defendant_data[\"defendant_aquited\"] = True \n",
    "    \n",
    "    if \"straff\" in raw_defendent_data : \n",
    "        defendant_data[\"punishment_raw\"] = raw_defendent_data[\"straff\"]\n",
    "\n",
    "    return pd.Series(defendant_data)\n",
    "\n",
    "\"\"\"\n",
    "Function for unpacking them raw punishment data into a set of variables\n",
    "\"\"\"\n",
    "def parse_punishment_data(raw_punishment_data:dict) : \n",
    "    new_punishment_columns = {\"betinget\":None,\n",
    "                              \"fengsel_dager\":None,\n",
    "                              \"fengsel_dager_betinget\":None,\n",
    "                              \"fengsel_subsidiært_dager\":None,\n",
    "                              \"bot\":None,\n",
    "                              \"forvaring\":None,\n",
    "                              \"amfunnsstraff_gjennomføringstid_dager\":None,\n",
    "                              \"samfunnsstraff_timer\":None}\n",
    "\n",
    "    # Return if no punishment \n",
    "    if raw_punishment_data == None : \n",
    "        return pd.Series(new_punishment_columns)\n",
    "    \n",
    "    # Otherwise loop through our new columns and update them if there's a match\n",
    "    for col in new_punishment_columns : \n",
    "        if col in raw_punishment_data : \n",
    "            new_punishment_columns[col] = raw_punishment_data[col]\n",
    "    \n",
    "    return pd.Series(new_punishment_columns)\n",
    "\n",
    "# Find our matching defendant data and unpack it. \n",
    "merged_df[\"raw_matched_defendant_data\"] = merged_df.apply(match_defendant_data,axis=1)\n",
    "merged_df[[\"convicted_raw\",\"defendant_aquitted\",\"punishment_raw\"]] = merged_df[\"raw_matched_defendant_data\"].apply(\n",
    "    parse_defendant_data\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "We reuse the approach for indictments for flattening and dummifying conviction sections\n",
    "\"\"\"\n",
    "# Flatten sections in conviction \n",
    "merged_df[\"flattened_convictions\"] = merged_df[\"dømt\"].apply(flatten_judgement_sections)\n",
    "\n",
    "# Explode it so we have all sections in the database in one place and take unique\n",
    "section_counts = merged_df[\"flattened_convictions\"].explode().value_counts()\n",
    "\n",
    "# Filter for all values above threshold (Note: Set to 0 for now because we want to include everything)\n",
    "filtered_values = section_counts[section_counts > 0].index.tolist()\n",
    "\n",
    "# Turn all sections with a count above the threshold into a dummy variable \n",
    "for section in filtered_values:\n",
    "    merged_df[\"convicted_\" + section] = merged_df['flattened_convictions'].apply(\n",
    "         dummify_sections,args=(section,)\n",
    "    ).astype(int)\n",
    "\n",
    "# Finally we unpack the punishment data \n",
    "merged_df[[\"betinget\",\n",
    "          \"fengsel_dager\",\n",
    "          \"fengsel_dager_betinget\",\n",
    "          \"fengsel_subsidiært_dager\",\n",
    "          \"bot\",\n",
    "          \"forvaring\",\n",
    "          \"amfunnsstraff_gjennomføringstid_dager\",\n",
    "          \"samfunnsstraff_timer\"]] = merged_df[\"punishment_raw\"].apply(parse_punishment_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay Judges\n",
    "\n",
    "First we unpack the lay judge field in the case data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Parse the data for the lay judge field in the case data. \n",
    "\"\"\"\n",
    "def parse_lay_judges(raw_lay_data:list) -> pd.Series : \n",
    "\n",
    "    parsed_lay_data = []\n",
    "    \n",
    "    # Loop through the lay judges\n",
    "    for lay_dict in raw_lay_data : \n",
    "\n",
    "        # Append the id field \n",
    "        if \"meddommer_id\" in lay_dict : \n",
    "            if \"$oid\" in lay_dict[\"meddommer_id\"] : \n",
    "                parsed_lay_data.append(lay_dict[\"meddommer_id\"][\"$oid\"])\n",
    "        \n",
    "        else : \n",
    "            parsed_lay_data.append(None)\n",
    "\n",
    "        # Whether the lay judge is in the majority or minority \n",
    "        if \"flertall_eller_mindretall\" in lay_dict : \n",
    "            if lay_dict[\"flertall_eller_mindretall\"] == \"flertall\" : \n",
    "                parsed_lay_data.append(\"Majority\")\n",
    "            \n",
    "            else : \n",
    "                parsed_lay_data.append(\"Minority\")\n",
    "        \n",
    "        else : \n",
    "            parsed_lay_data.append(None)\n",
    "    \n",
    "    return pd.Series(parsed_lay_data)\n",
    "\n",
    "\n",
    "merged_df[[\"lay_judge_1\",\"lay_judge_1_majority_or_minority\",\"lay_judge_2\",\"lay_judge_2_majority_or_minority\"]] = merged_df[\"meddommere\"].apply(\n",
    "    parse_lay_judges\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Lay Judges Dataframe \n",
    "\n",
    "Finally, we merge the lay judges database into a final, merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lay_judges(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "\n",
    "    return df\n",
    "\n",
    "lay_df = load_lay_judges(os.getcwd() + \"/00_raw_data/hidden/lr_meddommere.json\")\n",
    "\n",
    "# Extract ID for defendants \n",
    "lay_df[\"_id\"] = lay_df[\"_id\"].apply(lambda id_dict: id_dict[\"$oid\"])\n",
    "\n",
    "# Create copies for each lay df with the correct prefix \n",
    "lay_df_1 = lay_df.copy().add_prefix(\"lay_1_\")\n",
    "lay_df_2 = lay_df.copy().add_prefix(\"lay_2_\")\n",
    "\n",
    "# Merge based on lay judge 1 and lay judge 2 ids\n",
    "merged_df = pd.merge(merged_df, lay_df_1, left_on='lay_judge_1', right_on='lay_1__id', how='left')\n",
    "merged_df = pd.merge(merged_df, lay_df_2, left_on='lay_judge_2', right_on='lay_2__id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sections to legal groupings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_chapters(row_convictions:list,section:str) -> bool : \n",
    "    if isinstance(row_convictions,float) : \n",
    "        return False \n",
    "    \n",
    "    elif section in row_convictions : \n",
    "        return True\n",
    "\n",
    "    else : \n",
    "        return False     \n",
    "\n",
    "penal_code = pd.read_excel(\"01_preprocessed_data/penal_code_key.xlsx\")\n",
    "\n",
    "# Create mapping between Chapters and Sections \n",
    "chapter_section_mapping = dict(zip(penal_code['Section Norwegian'], penal_code['Chapter']))\n",
    "\n",
    "# Turn all Chapters into a dummy variable for convicted\n",
    "for section,chapter in chapter_section_mapping.items():\n",
    "\n",
    "    # Create dummy variables for Chapters related to charges \n",
    "    merged_df[\"chapter_charged_\" + chapter.lower().replace(\" \",\"_\")] = merged_df['flattened_charges'].apply(\n",
    "         dummify_chapters,args=(section,)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Create dummy variables for Chapters related to conviction  \n",
    "    merged_df[\"chapter_convicted_\" + chapter.lower().replace(\" \",\"_\")] = merged_df['flattened_convictions'].apply(\n",
    "         dummify_chapters,args=(section,)\n",
    "    ).astype(int)\n",
    "\n",
    "# Create mapping between English and Norwegian sections \n",
    "norwegian_english_mapping = dict(zip(penal_code['Section Norwegian'], penal_code['Section Name']))\n",
    "\n",
    "# Add Prefixes \n",
    "mapping_charged = {'charged_' + str(key): \"charged_\" + value.replace(\" \",\"_\").lower().replace(\",\",\"\") \\\n",
    " for key, value in norwegian_english_mapping.items()\n",
    " }\n",
    "\n",
    "mapping_convicted = {\n",
    "    'convicted_' + str(key): \"convicted_\" + value.replace(\" \",\"_\").lower().replace(\",\",\"\") \\\n",
    "    for key, value in norwegian_english_mapping.items()\n",
    "    }\n",
    "\n",
    "mapping_merged = {**mapping_charged,**mapping_convicted}\n",
    "merged_df.rename(columns=mapping_merged,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Intermediate Columns, Rename, and then Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8175, 1558)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A key mapping Norwegian columns to English translations \n",
    "column_key = eval(open(\"00_raw_data/en_no_column_key.txt\",\"r\").read())\n",
    "\n",
    "columns_to_drop = [\"fagdommere\",\n",
    "                   \"meddommere\",\n",
    "                   \"tiltale\",\n",
    "                   \"dom\",\n",
    "                   \"tiltalte\",\n",
    "                   \"dømt\",\n",
    "                   \"frifunnet\",\n",
    "                   \"uvisst\",\n",
    "                   \"raw_matched_defendant_data\",\n",
    "                   \"convicted_raw\",\n",
    "                   \"punishment_raw\",\n",
    "                   \"index\"]\n",
    "\n",
    "\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "merged_df.drop(columns=columns_to_drop,inplace=True)\n",
    "merged_df.rename(columns=column_key,inplace=True)\n",
    "\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.to_excel(\"01_preprocessed_data/hidden/preprocessed_data.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
