{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script: Preprocess Data \n",
    "Author: Gabriel Geiger\n",
    "Date: 17-01-2024 \n",
    "<br>\n",
    "\n",
    "Description: <br>\n",
    "@input: Raw json files representing databases containing judgements, defendants and lay judges. \n",
    "@output: A merged dataframe where all nested data is flattened. Each row is a defendant with their corresponding case data (outcome, lay judges etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import warnings\n",
    "import numpy as np \n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Get the notebook's directory\n",
    "notebook_path = os.path.abspath('')+'/..'\n",
    "os.chdir(notebook_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/justin-casimirbraun/GitHub/norwegian_criminal_justice\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Defendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defendant dataframe shape (8094, 24)\n"
     ]
    }
   ],
   "source": [
    "def load_defendants(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "    return df\n",
    "\n",
    "defendant_df = load_defendants(os.getcwd() + \"/00_raw_data/hidden/lr_tiltalte.json\")\n",
    "\n",
    "# Extract ID for defendants \n",
    "defendant_df[\"_id\"] = defendant_df[\"_id\"].apply(lambda id_dict: id_dict[\"$oid\"])\n",
    "\n",
    "# Add defendant prefix to all columns to avoid column collision with lay judges \n",
    "defendant_df = defendant_df.add_prefix(\"defendant_\")\n",
    "\n",
    "\n",
    "print(\"Defendant dataframe shape\",defendant_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Dataframe shape (9019, 8)\n"
     ]
    }
   ],
   "source": [
    "def load_cases(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "\n",
    "    return df\n",
    "\n",
    "cases_df = load_cases(os.getcwd() + \"/00_raw_data/hidden/lr_dommer.json\")\n",
    "\n",
    "# Extract id for cases \n",
    "cases_df[\"_id\"] = cases_df[\"_id\"].apply(\n",
    "    lambda id : id[\"$oid\"]\n",
    ")\n",
    "\n",
    "print(\"Case Dataframe shape\",cases_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case Data Validation \n",
    "\n",
    "Run data validation steps and throw out any rows that violate business rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 cases have been dropped because they did not have a judgement.\n",
      "127 cases have been dropped because they did not have one judge.\n",
      "36 cases have been dropped because they did not have two lay judges.\n",
      "11 cases have been dropped because they did not have a defendant.\n",
      "Shape (8767, 9)\n"
     ]
    }
   ],
   "source": [
    "# All cases should have a judgement, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"dom\"],inplace=True)\n",
    "print(\"{n} cases have been dropped because they did not have a judgement.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have one judge, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"fagdommere\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"fagdommere\"].apply(\n",
    "    lambda judge_array: len(judge_array) == 1)\n",
    "]\n",
    "\n",
    "# There appears to have been some parsing on the Norwegians-side. \n",
    "cases_df = cases_df[cases_df[\"fagdommere\"].apply(\n",
    "    lambda judge_array : type(judge_array) == list\n",
    ")]\n",
    "print(\"{n} cases have been dropped because they did not have one judge.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have two lay judges, drop any that do not. \n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"meddommere\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"meddommere\"].apply(\n",
    "    lambda lay_array: len(lay_array) == 2)\n",
    "]\n",
    "print(\"{n} cases have been dropped because they did not have two lay judges.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# All cases should have at least one defendant\n",
    "length_before = len(cases_df)\n",
    "cases_df.dropna(subset=[\"tiltalte\"],inplace=True)\n",
    "cases_df = cases_df[cases_df[\"tiltalte\"].apply(\n",
    "    lambda lay_array: len(lay_array) > 0)\n",
    "]\n",
    "print(\"{n} cases have been dropped because they did not have a defendant.\".format(n=length_before - len(cases_df)))\n",
    "\n",
    "# Reset the index. \n",
    "cases_df.reset_index(inplace=True)\n",
    "\n",
    "print(\"Shape\",cases_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match Cases with Courts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/vgss5c890n11fcs6qdgphwfw0000gn/T/ipykernel_12373/250446799.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  court_df[\"District Court\"].fillna(court_df[\"court_code\"],inplace=True)\n"
     ]
    }
   ],
   "source": [
    "court_df = pd.read_json(\"00_raw_data/hidden/lr_filnavn.json\")\n",
    "court_df[\"_id\"] = court_df[\"_id\"].apply(lambda id_dict : id_dict[\"$oid\"])\n",
    "\n",
    "court_df[\"court_code\"] = court_df[\"filnavn\"].apply( \n",
    "    lambda filename : filename.split(\"-\")[2].upper()\n",
    ")\n",
    "\n",
    "court_names = pd.read_csv(\"02_config/shortnames_districtcourts.csv\",delimiter=\";\")\n",
    "court_names.rename(columns={\"Short name\":\"court_code\"},inplace=True)\n",
    "\n",
    "court_df = pd.merge(court_df,court_names,on=\"court_code\",how=\"left\")\n",
    "court_df[\"District Court\"].fillna(court_df[\"court_code\"],inplace=True)\n",
    "\n",
    "court_df[\"District Court\"] = court_df[\"District Court\"].apply(\n",
    "    lambda court_name : court_name if \"tingrett\" in court_name else \"Unknown\"\n",
    ")\n",
    "court_df.rename(columns={\"District Court\":\"case_district_court\"},inplace=True)\n",
    "\n",
    "cases_df = pd.merge(cases_df,court_df,on=\"_id\",how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Case DataFrame with Defendants DataFrame \n",
    "\n",
    "Each defendant in the defendant database has an ID. Each case in the case dataframe has an array of defendant ids. Ultimately we want each row in our final dataframe to be a defendant and their corresponding case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1113 rows have been dropped because no matching defendant was found.\n",
      "Shape (8175, 37)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Takes the raw \n",
    "\"\"\"\n",
    "def parse_defendant_ids(raw_defendant_data:dict) -> list : \n",
    "    defendant_ids = []\n",
    "\n",
    "    for defendant in raw_defendant_data : \n",
    "        if \"motpart_id\" in defendant : \n",
    "            if \"$oid\" in defendant[\"motpart_id\"] : \n",
    "                defendant_ids.append(defendant[\"motpart_id\"]['$oid'])\n",
    "        \n",
    "    \n",
    "    return defendant_ids\n",
    "        \n",
    "# Create a clean list of defendant ids associated with the case \n",
    "cases_df[\"defendant_ids\"] = cases_df[\"tiltalte\"].apply(parse_defendant_ids)\n",
    "\n",
    "# Explode the 'defendant_ids' column in cases_df to create separate rows for each defendant\n",
    "cases_exploded = cases_df.explode('defendant_ids')\n",
    "\n",
    "# Merge the exploded cases_df with defendants_df based on defendant_id\n",
    "merged_df = pd.merge(cases_exploded, defendant_df, left_on='defendant_ids', right_on='defendant__id', how='left',suffixes=('_case', '_defendant'))\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Drop all cases without a defendant. Unfortunately not every defendant has an ID \n",
    "length_before = len(merged_df)\n",
    "merged_df.dropna(subset=[\"defendant__id\"],inplace=True)\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "print(\"{i} rows have been dropped because no matching defendant was found.\".format(i=length_before - len(merged_df)))\n",
    "\n",
    "print(\"Shape\",merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Judgement Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dom column\n",
    "judgement_df = pd.json_normalize(data=merged_df[\"dom\"],meta=[\"dømt\",\"frifunnet\",\"uvisst\"])\n",
    "\n",
    "# Create a dummy variable for frifunnet (aquitted) \n",
    "judgement_df[\"judgement_aquitted\"] = judgement_df[\"frifunnet\"].apply(\n",
    "    lambda a : False if isinstance(a,float) else True)\n",
    "\n",
    "# Create a dummy variable for uvisst (unknown) \n",
    "judgement_df[\"judgement_unknown\"] = judgement_df[\"uvisst\"].apply(\n",
    "    lambda u : False if isinstance(u,float) else True)\n",
    "\n",
    "# Create a dummy variable for dømt (convicted)\n",
    "judgement_df[\"judgement_convicted\"] = judgement_df[\"dømt\"].apply(\n",
    "    lambda u : False if isinstance(u,float) else True)\n",
    "\n",
    "# Merge the dataframes \n",
    "length_before_merge = len(merged_df)\n",
    "merged_df = pd.merge(merged_df, judgement_df, left_index=True, right_index=True, how='inner')\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# Make sure no merging errors \n",
    "assert length_before_merge == len(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profesional Judges \n",
    "\n",
    "The only variable here behind function (which is always just profesional judge) is whether the profesional judge is or isn't in the majority. Note, this variable is n/a if the decision is unanimous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_judge_data(raw_judge_data:list) : \n",
    "    judge_dict = raw_judge_data[0]\n",
    "\n",
    "    if \"flertall_eller_mindretall\" in judge_dict : \n",
    "        if judge_dict[\"flertall_eller_mindretall\"] == \"flertall\" : \n",
    "            return \"Majority\"\n",
    "\n",
    "        else : \n",
    "            return \"Minority\"\n",
    "    \n",
    "    else : \n",
    "        return None \n",
    "\n",
    "merged_df[\"judge_majority_or_minority\"] = merged_df[\"fagdommere\"].apply(parse_judge_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indictment\n",
    "\n",
    "The sections under the indictment are in nested json of an arbitrary depth, so it's a bit tricky to unpack it all. Ultimately, we want to turn all indictment sections into dummy variables. \n",
    "\n",
    "An important note: Not all the sections in the indictment nessicairly pertain to all defendants in the case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profesional Judges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_profesional_judges(filename) : \n",
    "    return pd.read_json(filename)\n",
    "\n",
    "def load_judge_to_case(filename) : \n",
    "    return pd.read_json(filename)\n",
    "\n",
    "def parse_judge_id(id) : \n",
    "    if isinstance(id,float) : \n",
    "        return None \n",
    "    \n",
    "    if len(id) == 0 :\n",
    "        return None \n",
    "    \n",
    "    if id == [{}] : \n",
    "        return None \n",
    "    \n",
    "    if \"$oid\" not in id[0]['fagdommer_id'] : \n",
    "        return None \n",
    "    \n",
    "    else : \n",
    "        return id[0]['fagdommer_id'][\"$oid\"]\n",
    "\n",
    "\n",
    "judge_case_df = load_judge_to_case(os.getcwd() + \"/00_raw_data/hidden/lr_dommer_fagdommerid.json\")\n",
    "judge_df = load_profesional_judges(os.getcwd() + \"/00_raw_data/hidden/lr_fagdommere.json\")\n",
    "\n",
    "judge_case_df[\"case_id\"] = judge_case_df[\"_id\"].apply(\n",
    "    lambda id : id[\"$oid\"]\n",
    ")\n",
    "\n",
    "judge_case_df[\"judge_id\"] = judge_case_df[\"fagdommere\"].apply(\n",
    "    parse_judge_id\n",
    ")\n",
    "\n",
    "judge_df[\"judge_id\"] = judge_df[\"_id\"].apply(\n",
    "    lambda id : id[\"$oid\"]\n",
    ")\n",
    "\n",
    "# Merge our two dataframes\n",
    "judge_df = pd.merge(judge_case_df,judge_df,on=\"judge_id\",how=\"left\")\n",
    "judge_df.drop(columns=[\"_id_x\",\"_id_y\",\"fagdommere\"],inplace=True)\n",
    "judge_df = judge_df.add_prefix(\"professional_judge_\")\n",
    "\n",
    "# Merge with our main dataframe \n",
    "merged_df = pd.merge(merged_df,judge_df,left_on=\"_id\",right_on=\"professional_judge_case_id\",how=\"left\")\n",
    "merged_df.drop(columns=[\"professional_judge_case_id\"],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Recursively flatten the nested json \n",
    "\"\"\"\n",
    "def flatten_judgement_sections(nested_list) -> list:\n",
    "    if (isinstance(nested_list,float)) or (nested_list == None) : \n",
    "        return nested_list \n",
    "\n",
    "    # If the judgement is unknown it is labeled \"Fant ikke ord\"\n",
    "    if isinstance(nested_list,str) : \n",
    "        return [nested_list]\n",
    "    \n",
    "    flattened_list = []\n",
    "    \n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_judgement_sections(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "\n",
    "    return flattened_list\n",
    "\n",
    "def dummify_sections(row_sections:list,section:str) -> bool : \n",
    "    if isinstance(row_sections,float) : \n",
    "        return False \n",
    "    \n",
    "    elif section in row_sections : \n",
    "        return True \n",
    "\n",
    "    else : \n",
    "        return False \n",
    "    \n",
    "# Flatten sections in tiltale (indictment)\n",
    "merged_df[\"flattened_charges\"] = merged_df[\"tiltale\"].apply(flatten_judgement_sections)\n",
    "\n",
    "# Explode it so we have all sections in the database in one place and take unique\n",
    "section_counts = merged_df[\"flattened_charges\"].explode().value_counts()\n",
    "\n",
    "# Filter for all values above threshold (Note: Set to 0 for now because we want to include everything)\n",
    "filtered_values = section_counts[section_counts > 0].index.tolist()\n",
    "\n",
    "# Turn all sections with a count above the threshold into a dummy variable (For now all sections)\n",
    "for section in filtered_values:\n",
    "    merged_df[\"charged_\" + section] = merged_df['flattened_charges'].apply(\n",
    "         dummify_sections,args=(section,)\n",
    "    ).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Aggravating and Mitigating Circumstances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Recursively flatten the nested json \n",
    "\"\"\"\n",
    "def flatten_circumstances(nested_list) -> list:\n",
    "    if (isinstance(nested_list,float)) or (nested_list == None) : \n",
    "        return nested_list \n",
    "\n",
    "    # If the judgement is unknown it is labeled \"Fant ikke ord\"\n",
    "    if isinstance(nested_list,str) : \n",
    "        return [nested_list]\n",
    "    \n",
    "    flattened_list = []\n",
    "    \n",
    "    for item in nested_list:\n",
    "        if isinstance(item, list):\n",
    "            flattened_list.extend(flatten_circumstances(item))\n",
    "        else:\n",
    "            flattened_list.append(item)\n",
    "\n",
    "    return flattened_list\n",
    "\n",
    "def dummify_circumstances(row_circumstances:list,circumstance:str) -> bool : \n",
    "    if isinstance(row_circumstances,float) : \n",
    "        return False \n",
    "    \n",
    "    elif circumstance in row_circumstances : \n",
    "        return True \n",
    "\n",
    "    else : \n",
    "        return False \n",
    "\n",
    "\n",
    "aggravating_mitigating = pd.read_json(\"00_raw_data/hidden/130524_judgements_mitigating_aggravating.json\")\n",
    "\n",
    "aggravating_mitigating['_id'] = aggravating_mitigating['_id'].apply(\n",
    "    lambda id : id['$oid']\n",
    ")\n",
    "\n",
    "# Create boolean variable that captures whether a case has aggravating or mitigating circumstances \n",
    "aggravating_mitigating['has_aggravating'] = aggravating_mitigating['skjerpende'].apply(\n",
    "    lambda agg : True if isinstance(agg,list) else False \n",
    ")\n",
    "\n",
    "aggravating_mitigating['has_mitigating'] = aggravating_mitigating['formildende'].apply(\n",
    "    lambda mit : True if isinstance(mit,list) else False \n",
    ")\n",
    "\n",
    "# Flatten aggravating and mitigating circumstnaces \n",
    "\n",
    "# Flatten sections in tiltale (indictment)\n",
    "aggravating_mitigating[\"flattened_aggravating\"] = aggravating_mitigating[\"skjerpende\"].apply(flatten_circumstances)\n",
    "\n",
    "aggravating_mitigating['flattened_mitigating'] = aggravating_mitigating[\"formildende\"].apply(flatten_circumstances)\n",
    "\n",
    "aggravating_list = aggravating_mitigating[\"flattened_aggravating\"].explode().unique().tolist()\n",
    "mitigating_list = aggravating_mitigating[\"flattened_mitigating\"].explode().unique().tolist()\n",
    "\n",
    "for circumstance in aggravating_list:\n",
    "    if isinstance(circumstance,float) : \n",
    "        continue \n",
    "\n",
    "    aggravating_mitigating[\"aggravating_\" + circumstance] = aggravating_mitigating['flattened_aggravating'].apply(\n",
    "         dummify_circumstances,args=(circumstance,)\n",
    "    ).astype(int)\n",
    "\n",
    "for circumstance in mitigating_list:\n",
    "    if isinstance(circumstance,float) : \n",
    "        continue \n",
    "\n",
    "    aggravating_mitigating[\"mitigating_\" + circumstance] = aggravating_mitigating['flattened_mitigating'].apply(\n",
    "         dummify_circumstances,args=(circumstance,)\n",
    "    ).astype(int)\n",
    "\n",
    "\n",
    "merged_df = pd.merge(merged_df, aggravating_mitigating, on=\"_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching Defendant Data\n",
    "\n",
    "The case data contains an array called 'defendants' with some information about each defendant in the case (punishment, whether they were aquitted, etc.). For our purposes, we only want each row to have the relevant case defendant data and punishment, not the data for all the other defendants in the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wg/vgss5c890n11fcs6qdgphwfw0000gn/T/ipykernel_12373/1715876295.py:68: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df.convicted_raw.fillna(value=np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Matches the nested defendant data with the defendant matched to this row. \n",
    "\"\"\"\n",
    "def match_defendant_data(row) -> dict : \n",
    "    defendant_list = row[\"tiltalte\"]\n",
    "\n",
    "    for defendant in defendant_list: \n",
    "        if \"motpart_id\" in defendant : \n",
    "            if defendant[\"motpart_id\"][\"$oid\"] == row[\"defendant__id\"] : \n",
    "                return defendant\n",
    "\n",
    "\"\"\"\n",
    "Function for flattening the nested json data related to the defendant. \n",
    "Creates two intermediate variables: convicted_raw and punishment_raw that are unpacked \n",
    "seperately \n",
    "\"\"\"\n",
    "def parse_defendant_data(raw_defendent_data:dict) : \n",
    "    defendant_data = {\"convicted_raw\":None,\n",
    "                      \"defendant_aquited\":None,\n",
    "                      \"punishment_raw\":None}\n",
    "\n",
    "    if \"dømt\" in raw_defendent_data : \n",
    "        defendant_data[\"convicted_raw\"] = raw_defendent_data[\"dømt\"]\n",
    "    \n",
    "    if \"frifunnet\" in raw_defendent_data : \n",
    "        defendant_data[\"defendant_aquited\"] = True \n",
    "    \n",
    "    if \"straff\" in raw_defendent_data : \n",
    "        defendant_data[\"punishment_raw\"] = raw_defendent_data[\"straff\"]\n",
    "\n",
    "    return pd.Series(defendant_data)\n",
    "\n",
    "\"\"\"\n",
    "Function for unpacking them raw punishment data into a set of variables\n",
    "\"\"\"\n",
    "def parse_punishment_data(raw_punishment_data:dict) : \n",
    "    new_punishment_columns = {\"betinget\":None,\n",
    "                              \"fengsel_dager\":None,\n",
    "                              \"fengsel_dager_betinget\":None,\n",
    "                              \"fengsel_subsidiært_dager\":None,\n",
    "                              \"bot\":None,\n",
    "                              \"forvaring\":None,\n",
    "                              \"amfunnsstraff_gjennomføringstid_dager\":None,\n",
    "                              \"samfunnsstraff_timer\":None}\n",
    "\n",
    "    # Return if no punishment \n",
    "    if raw_punishment_data == None : \n",
    "        return pd.Series(new_punishment_columns)\n",
    "    \n",
    "    # Otherwise loop through our new columns and update them if there's a match\n",
    "    for col in new_punishment_columns : \n",
    "        if col in raw_punishment_data : \n",
    "            new_punishment_columns[col] = raw_punishment_data[col]\n",
    "    \n",
    "    return pd.Series(new_punishment_columns)\n",
    "\n",
    "# Find our matching defendant data and unpack it. \n",
    "merged_df[\"raw_matched_defendant_data\"] = merged_df.apply(match_defendant_data,axis=1)\n",
    "merged_df[[\"convicted_raw\",\"defendant_aquitted\",\"punishment_raw\"]] = merged_df[\"raw_matched_defendant_data\"].apply(\n",
    "    parse_defendant_data\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "We reuse the approach for indictments for flattening and dummifying conviction sections\n",
    "\"\"\"\n",
    "# Flatten sections in conviction \n",
    "\n",
    "merged_df.convicted_raw.fillna(value=np.nan, inplace=True)\n",
    "\n",
    "merged_df[\"flattened_convictions\"] = merged_df[\"convicted_raw\"].apply(flatten_judgement_sections)\n",
    "\n",
    "# Explode it so we have all sections in the database in one place and take unique\n",
    "section_counts = merged_df[\"flattened_convictions\"].explode().unique()\n",
    "\n",
    "# Turn all sections with a count above the threshold into a dummy variable \n",
    "for section in section_counts:\n",
    "    if isinstance(section,str) : \n",
    "        merged_df[\"convicted_\" + section] = merged_df['flattened_convictions'].apply(\n",
    "            dummify_sections,args=(section,)\n",
    "        ).astype(int)\n",
    "\n",
    "# Finally we unpack the punishment data \n",
    "merged_df[[\"betinget\",\n",
    "          \"fengsel_dager\",\n",
    "          \"fengsel_dager_betinget\",\n",
    "          \"fengsel_subsidiært_dager\",\n",
    "          \"bot\",\n",
    "          \"forvaring\",\n",
    "          \"amfunnsstraff_gjennomføringstid_dager\",\n",
    "          \"samfunnsstraff_timer\"]] = merged_df[\"punishment_raw\"].apply(parse_punishment_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lay Judges\n",
    "\n",
    "First we unpack the lay judge field in the case data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Parse the data for the lay judge field in the case data. \n",
    "\"\"\"\n",
    "def parse_lay_judges(raw_lay_data:list) -> pd.Series : \n",
    "\n",
    "    parsed_lay_data = []\n",
    "    \n",
    "    # Loop through the lay judges\n",
    "    for lay_dict in raw_lay_data : \n",
    "\n",
    "        # Append the id field \n",
    "        if \"meddommer_id\" in lay_dict : \n",
    "            if \"$oid\" in lay_dict[\"meddommer_id\"] : \n",
    "                parsed_lay_data.append(lay_dict[\"meddommer_id\"][\"$oid\"])\n",
    "        \n",
    "        else : \n",
    "            parsed_lay_data.append(None)\n",
    "\n",
    "        # Whether the lay judge is in the majority or minority \n",
    "        if \"flertall_eller_mindretall\" in lay_dict : \n",
    "            if lay_dict[\"flertall_eller_mindretall\"] == \"flertall\" : \n",
    "                parsed_lay_data.append(\"Majority\")\n",
    "            \n",
    "            else : \n",
    "                parsed_lay_data.append(\"Minority\")\n",
    "        \n",
    "        else : \n",
    "            parsed_lay_data.append(None)\n",
    "    \n",
    "    return pd.Series(parsed_lay_data)\n",
    "\n",
    "\n",
    "merged_df[[\"lay_judge_1\",\"lay_judge_1_majority_or_minority\",\"lay_judge_2\",\"lay_judge_2_majority_or_minority\"]] = merged_df[\"meddommere\"].apply(\n",
    "    parse_lay_judges\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Lay Judges Dataframe \n",
    "\n",
    "Finally, we merge the lay judges database into a final, merged dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lay_judges(file:str) -> pd.DataFrame : \n",
    "    df = pd.read_json(file)\n",
    "\n",
    "    return df\n",
    "\n",
    "lay_df = load_lay_judges(os.getcwd() + \"/00_raw_data/hidden/lr_meddommere.json\")\n",
    "\n",
    "# Extract ID for defendants \n",
    "lay_df[\"_id\"] = lay_df[\"_id\"].apply(lambda id_dict: id_dict[\"$oid\"])\n",
    "\n",
    "# Create copies for each lay df with the correct prefix \n",
    "lay_df_1 = lay_df.copy().add_prefix(\"lay_1_\")\n",
    "lay_df_2 = lay_df.copy().add_prefix(\"lay_2_\")\n",
    "\n",
    "# Merge based on lay judge 1 and lay judge 2 ids\n",
    "merged_df = pd.merge(merged_df, lay_df_1, left_on='lay_judge_1', right_on='lay_1__id', how='left')\n",
    "merged_df = pd.merge(merged_df, lay_df_2, left_on='lay_judge_2', right_on='lay_2__id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match sections to legal groupings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify_chapters(row_convictions:list,section:str) -> bool : \n",
    "    if isinstance(row_convictions,float) : \n",
    "        return False \n",
    "    \n",
    "    elif section in row_convictions : \n",
    "        return True\n",
    "\n",
    "    else : \n",
    "        return False     \n",
    "\n",
    "penal_code = pd.read_excel(\"02_config/penal_code_key.xlsx\")\n",
    "\n",
    "# Create mapping between Chapters and Sections \n",
    "chapter_section_mapping = dict(zip(penal_code['Section Norwegian'], penal_code['Chapter']))\n",
    "\n",
    "# Turn all Chapters into a dummy variable for convicted\n",
    "for section,chapter in chapter_section_mapping.items():\n",
    "\n",
    "    # Create dummy variables for Chapters related to charges \n",
    "    merged_df[\"chapter_charged_\" + chapter.lower().replace(\" \",\"_\")] = merged_df['flattened_charges'].apply(\n",
    "         dummify_chapters,args=(section,)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Create dummy variables for Chapters related to conviction  \n",
    "    merged_df[\"chapter_convicted_\" + chapter.lower().replace(\" \",\"_\")] = merged_df['flattened_convictions'].apply(\n",
    "         dummify_chapters,args=(section,)\n",
    "    ).astype(int)\n",
    "\n",
    "# Create mapping between English and Norwegian sections \n",
    "norwegian_english_mapping = dict(zip(penal_code['Section Norwegian'], penal_code['section_name']))\n",
    "\n",
    "# Add Prefixes \n",
    "mapping_charged = {'charged_' + str(key): \"charged_\" + value.replace(\" \",\"_\").lower().replace(\",\",\"\") \\\n",
    " for key, value in norwegian_english_mapping.items()\n",
    " }\n",
    "\n",
    "mapping_convicted = {\n",
    "    'convicted_' + str(key): \"convicted_\" + value.replace(\" \",\"_\").lower().replace(\",\",\"\") \\\n",
    "    for key, value in norwegian_english_mapping.items()\n",
    "    }\n",
    "\n",
    "mapping_merged = {**mapping_charged,**mapping_convicted}\n",
    "merged_df.rename(columns=mapping_merged,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Intermediate Columns, Rename, and then Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8175, 3057)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# A key mapping Norwegian columns to English translations \n",
    "column_key = eval(open(\"02_config/en_no_column_key.txt\",\"r\").read())\n",
    "\n",
    "columns_to_drop = [\"fagdommere\",\n",
    "                   \"meddommere\",\n",
    "                   \"tiltale\",\n",
    "                   \"dom\",\n",
    "                   \"tiltalte\",\n",
    "                   \"dømt\",\n",
    "                   \"frifunnet\",\n",
    "                   \"uvisst\",\n",
    "                   \"raw_matched_defendant_data\",\n",
    "                   \"convicted_raw\",\n",
    "                   \"punishment_raw\",\n",
    "                   \"index\"]\n",
    "\n",
    "\n",
    "merged_df.reset_index(inplace=True,drop=True)\n",
    "merged_df.drop(columns=columns_to_drop,inplace=True)\n",
    "merged_df.rename(columns=column_key,inplace=True)\n",
    "\n",
    "print(merged_df.shape)\n",
    "\n",
    "merged_df.to_excel(\"01_preprocessed_data/hidden/preprocessed_data.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
